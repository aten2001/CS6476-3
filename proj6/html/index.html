<html>
<head>
<title>Deep Learning Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>M S Suraj</h1>
</div>
</div>
<div class="container">

<h2> Project 6 / Deep Learning</h2>

<div>
<h3>Part - 1: Training a deep network from scratch</h3>
<h4>0. Base Case: Running the code as provided</h4>
<p> I ran the skeleton Convolutional Neural Network(CNN) that was provided without any modifications. The lowest validation error I obtained in this case was 0.746 - an accuracy of <strong>25.4%</strong> which is pretty bad!</p>
<img src="img/part1_base.png" width="100%">
<img src="img/part1_base_filters.png" width="100%">
<p>As you can see in the above plots, the training error keeps decreasing to almost 0 at the end of training - however, the test set error (which also happens to be the validation set for this project) doesn't decrease after a point. This is because we are overfitting the network to our limited training data.</p>
</div>
<h4>1. Increasing training data size: Jitter!</h4>
<p>I implemented jittering by flipping(mirroring) a fraction(set to 50%, but changeable) of each of the training batch images. So effectively, I have a training data of size 3000 images now. Keeping all the other parameters same as before, I obtained a lowest validation error of 0.664 - an accuracy of <strong>33.6%</strong>, an increase of ~8% from our previous result. Changing the number of epochs from 30 to 40 or 60 didn't change the accuracy by more than 1%, so I am not presenting the result here.</p>
<img src="img/part1_with_jitter.png" width="100%">
<img src="img/part1_with_jitter_filters.png" width="100%">
<p>Note that on the right-hand side plot, the training error doesn't decrease as rapidly as in the previous case. In fact, it increases a bit toward the end. This is because we have reduced the overfitting by augmenting the training data.</p>
<h4>2. Zero centering the training images</h4>
The training images aren't zero-centered or normalized. So I apply zero centering by subtractin the mean from each of the training images. This improves the accuracy significantly. Without modifying any of the parameters, I obtained a lowest validation error of 0.472 - accuracy of <strong>52.8%</strong>, a significant increase of ~19% from our previous best.
<img src="img/part1_with_jitter_zc.png" width="100%">
<img src="img/part1_with_jitter_zc_filters.png" width="100%">
<h4>3. Add dropout regularization</h4>
<p>Since we have a limited dataset, I added dropout regularization. It is easy to do so by adding a dropout layer (after the 1st convolutional layer) to our network using the MatConvNet library. With a dropout rate of 0.5, a learning rate of 0.0001(same as previous) and 100 training epochs, I get a lowest validation error of 0.3893 - an accuracy of <strong>61.01%</strong>, an increase of ~8%.</p>
<img src="img/part1_with_jitter_zc_dropout.png" width="100%">
<img src="img/part1_with_jitter_zc_dropout_filters.png" width="100%">
<p>Note that the filters in our convolutional layers are much more stuctured than before.</p>
<h4>4. I need to go deeper! (or what did she say?)</h4>
<p>We seem to be losing valuable information from the convolutional layer due to the max-pooling layer since it downsamples the convolutional layer pretty heavily - ie, by a factor of 7. One way to fix this is to downsample it much more slowly across the network, by adding more convolutional layers (and max-pool layers) - this reduces the lossiness of the network. However, deeper networks need lot more data to get good results since now the network can be intuitively assumed to be approximating a more complex function and learning the correct weights would require more training data.<br>
I added one more Conv unit (a convolutional layer, a max-pooling layer and a ReLU layer) on top of the previous convolutional layer to make the network 3 layers deep. Note that, when counting the depth of the network I'm only counting the layers that have weights to be learned (so, ReLU, max-pooling and dropout layers are ignored). Here is the network configuration:<br><br>
<img src="img/part1_2_layers_diagram.png" width="100%">
Without augmenting the data any further (we already added random mirroring), with 100 epochs and a learning rate of 0.0001, I obtained a lowest validation error of 0.466 - an accuracy of <strong>53.4%</strong>.
<img src="img/part1_with_jitter_zc_dropout_2_layers.png" width="100%">
<img src="img/part1_with_jitter_zc_dropout_2_layers_filters.png" width="100%">

<h3>Part - 2: Fine-tuning an existing network</h3>
We fine-tune the VGG-F network and train it on our dataset of images. This essentially acts as a better intialization of the network weights than randomly doing so because the network implicitly has valuable information on scenes it learnt from.<br>
We modify the final layers of the VGG-F network to a fully connected layer that gives us a 15 way classification output. Here's the final structure of the network -
<img src="img/part2_base_network.png" width="100%">
With a learning rate of 0.0001 and 12 epochs (batch size of 50 images without any jittering or dropout), I got a lowest validation error of 0.11 - an accuracy of <strong>89%</strong>.<br>
<img src="img/part2_base_12.png" width="100%">
<img src="img/part2_base_filters.png" width="100%">
Note that on the error plot, you can see that the validation error is lower than the training error after the first epoch. This seems to be by chance - the VGG-F network weights probably classified the images in validation batch better than the ones in training batch. As we train the network on the new dataset, the training error decreases close to 0 as expected whilst the validation error still persists ~0.11.
</p>

<h3>Part - 3: Extra Credit - Training the VGG-F network on SUN attributes</h3>
<p>
It would be interestin to see how the VGG-F network can be adapted and tuned to predict scene attributes instead of categories. I downloaded the SUN attributes database from <a href="https://cs.brown.edu/~gen/sunattributes.html">here</a>. I modified the VGG-F network to output 102 attribute predictions instead of multi-class category predictions and uses a logistic log loss function (refer <a href="http://www.vlfeat.org/matconvnet/mfiles/vl_nnloss/">vl_nnloss()</a>).<br>
However due to timing limiatations, I couldn't complete the training and analysis of the results before the deadline - so I leave the code (proj6_part3.m) as it is.
</p>
</div>
</body>
</html>
