<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 960px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

td img {
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1><span style="color: #DE3737">M S Suraj</span></h1>
</div>
</div>
<div class="container">

<h2>Project 2: Local Feature Matching</h2>

<h3>Feature Detection: Harris-Stephen's corner detection algorithm</h3>
<p>
	For the feature detection, I decided to implement the Harris-Stephen's corner detection algorithm. The algorithm is very straightforward:
	<ul>
		<li>Generate the xy-gradients of the image, Ix and Iy.</li>
		<li>Smoothen the gradients with a small gaussian kernel.</li>
		<li>Generate the intermediate matrices for computing the Harris cornerness measure by taking products amongst the above gradient matrices.</li>
		<li>Convole these gradient-products with a larger Gaussian and apply thresholding to detect Harris corner points.</li>
		<li>Perform non-maximal suppression</li>
	</ul>
	For a slightly mathematical explanation, wikipedia has a good description of the <a href="https://en.wikipedia.org/wiki/Corner_detection#The_Harris_.26_Stephens_.2F_Plessey_.2F_Shi.E2.80.93Tomasi_corner_detection_algorithms">algorithm</a>. However, in addition I explanined some more details of it (for personal clarity) here on my personal <a href="http://braindeadpool.com/computer%20vision/2016/09/21/harris-corner-detection-eigenvector/">webpage</a><br/>

	The non-maximal suppression finds the local maxima points within an adaptive region around each potential corner point after thresholding - this ensures that we detect only the promiment corner points and not the neighboring points whose gradient magnitudes may also be above the threshold.<br/>
	<table border="1">
		<tr>
			<td><img src="img/notre_dame/harris.jpg"></td>
		</tr>
		<tr><td>The figure shows the most confident 500 points generated by the Harris corner detection algorithm on the Notre Dame test image.</td></tr>
	</table>
</p>
<h3>Feature Descriptor: SIFT-like feature</h3>
For describing the feature point, I implemented a SIFT like feature descriptor. Here's a brief description of the basic SIFT descriptor algorithm:<br>
<ul>
	<li>At each point, I generate the polar-image gradients (this can be speeded up by reusing the xy-gradients generated in feature detection part and converting them into polar coordinates).</li>
	<li>We take a patch of the image gradients 16-by-16 pixels wide centered at the feature point. (here the feature_width is 16 pixels) and divide it into 4-by-4 pixels wide sub-patches.</li>
	<li>Depending on the orientation of the gradient at each point in the sub-patch, we bin it into a histogram of 8 bins. Each bin represents an orientation generated by splitting the axis around the centre of the sub-patch into 8 equal parts of a circle.</li>
	<li>Note that, we increment the bin of the histogram corresponding to the orientation of the closest match amongst the 8 basis-orientations.</li>
	<li>Thus we now have 4-by-4 set of histograms, with each histogram having data for 8 orientations - thereby giving us a final feature vector of size 4x4x8 = 128</li>
</ul>
<h3>Feature Matching: Naive brute force comparison with NND</h3>
Now that we have a set of feature points and the correspoding descriptors for the two images, we need to match them and choose the best matches. This is done in a very naive way (without any speed optimization using kd-trees, etc). 
	<ul>
		<li>For each feature point of the first image, we compute the Euclidean distance in feature-space to every feature point in the second image and take the ratio of the nearest two feature points.</li> 
		<li>If the ratio is below a certain threshold, we consider the nearest feature point to be a valid point with a confidence inversely proportional to the ratio.</li>
	</ul>
For the test images, I evaluate and display the 100 most confident matches.
<h3>Extra stuff: Improved SIFT description</h3>
The simple binning algorithm essentially takes into account only the orientation of the gradients. In addition, it only considers closest match orientation for binning.<br/>
I decided to implement weighted binning taking into account the magnitudes of the gradients as well. Although it is suggested in the slides to distribute magnitude to the bins of the closest two orientations, it works better if you distribute it to all the bins with exponential weighting. Thus, every gradient vector contributes to all the 8-bins exponentially weighted to the inverse of the distance to between the gradient orientation and each of the basis orientations. Also, instead of trilinear interpolation, it works better to distribute a part of this weighted gradient magnitude to the neighbor histogram depending on the orientation of the gradient.
<h3>Some more results</h3>
<span>Notre Dame image pair</span>
<table border="1">
		<tr><td><img src="img/notre_dame/harris.jpg"></td></tr>
		<tr><td><img src="img/notre_dame/eval.jpg"></td></tr>
		<tr><td><img src="img/notre_dame/vis_arrows.jpg"></td></tr>
		<tr><td>Accuracy rate of 95%</td></tr>
</table>
<strong><span>Mount Rushmore image pair</span></strong>
<table border="1">
		<tr><td><img src="img/mount_rushmore/harris.jpg"></td></tr>
		<tr><td><img src="img/mount_rushmore/eval.jpg"></td></tr>
		<tr><td><img src="img/mount_rushmore/vis_arrows.jpg"></td></tr>
		<tr><td>Accuracy rate of 99%</td></tr>
</table>
<strong><span>Pantheon Paris image pair</span></strong>
<table border="1">
		<tr><td><img src="img/pantheon_paris/harris.jpg"></td></tr>
		<tr><td><img src="img/pantheon_paris/vis_arrows.jpg"></td></tr>
		<tr><td>Evaluation data not available</td></tr>
</table>
</div>
</body>
</html>
